% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------
 
\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
 
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
 
\newenvironment{abs}[2][Abstract]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2}]}{\end{trivlist}}

\newenvironment{p1}[2][Motivation]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2}]}{\end{trivlist}}

\newenvironment{p2}[2][Data]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2}]}{\end{trivlist}}

\newenvironment{p3}[2][Methods]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2}]}{\end{trivlist}}

\newenvironment{p4}[2][Results and Evaluation]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2}]}{\end{trivlist}}

\newenvironment{p5}[2][Discussion and Future Work]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2}]}{\end{trivlist}}

\newenvironment{p6}[2][Software Tools and Libraries]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2}]}{\end{trivlist}}

\newenvironment{p7}[2][References]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2}]}{\end{trivlist}}

\usepackage{graphicx}
\graphicspath{{./}}

\begin{document}
 
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------
 
\title{San Fancisco Crime Study}
\author{Yunzhong He (204010749) Zhinan Guan (004118232)}
\maketitle

\begin{abs}{}
\item{}
For this project we performed data minings and visualizations on San Francisco's crime data from 2003 to 2015, in order to find patterns in the space-time distributions of different types of crimes. We fisrt performed feature selection on district and street names using random forest, and obtained the most meanful names in terms of separating different kinds of crimes. Then we used Linear Discrimiant Analysis (LDA), with respect to time, GPS coordinate and the features selected in the previous step, to learn the likelihood function of different crimes. Eventually, using our learned results we were able to generate heat maps for differnt crimes throughout San Fransiciso.
\end{abs}

\begin{p1}{}
\item{}
\end{p1}

\begin{p2}{}
\item{}
\end{p2}

\begin{p3}{}
\item{\textbf{Feature Selection with Random Forrest\\}}
From the dataset we are given the district and street names of all the crime occurances. Using bag-of-word model[1] we can encode them into binary vectors as part of our feature. But given the amount of dataset, the computation is going to take very long time given the dimension of our feature space. More importantly, the resultant vector of bag-of-words encoding is very sparse -- for a vector of 500+ entries represting bag-of-words encoding of one data point, at most two entries will be 1, because there is only one district and one street name for each crime incident.\\\\
To address this issue, we want to filter out districts and streets that are not very informative in terms of crime classification. Limiting our bag-of-words model to contain only 10 words, we want to find a subset S, of the set of all districts and streets W, that maximizes the information gain. We we have
\begin{align*}
	S^* = argmax_{S \subset W} \sum_i^N log(p(x_i|S)) p(S)
\end{align*}
This is thus very similar to decision tree learning, except that we want to pick the top 10 features, rathern than picking one at each depth. We used an ensemble model, the Extremely Randomized Trees[2] to solve the problem. It basically add randomness in the splitting process, and combines all the results to give a feature ranking.
\end{p3}

\begin{p4}{}
\item{\textbf{Feature Selection Results}\\}
Running the feature selection process for districts and streets, below are the top 10 names we found.
\begin{center}
	\begin{tabular}{||c c c||} 
		\hline
	   	Name & Weight & Type\\
		\hline
		TENDERLOIN & 0.248802719147 & District\\
		\hline
		BAYVIEW  & 0.12442886283 & District\\
		\hline
		CENTRAL  & 0.164109496071 & District\\
		\hline
		SOUTHERN & 0.120383230634 & District\\
		\hline
		NORTHERN & 0.108657207712 & District\\
		\hline
		INGLESIDE & 0.0851453969923 & District\\
		\hline
		TARAVAL & 0.0434198382797 & District\\
		\hline
		MISSION AVE & 0.0410287072239 & Street\\
		\hline
		RICHMOND & 0.0328377255613 & District\\
		\hline
		PARK & 0.0311868155498 & District\\
		\hline
	\end{tabular}
	{\\fig.1 best names}
\end{center}
From the table we can see that in general districts are more informative than streets. And since we are using dicision trees, this should not be a result of districts having higher frequencies to appear in our dataset than streets. We further normalized our data and ran the same algorithm, and obtained the same results. But notice that Mission Ave is more informative than some districts, which is an interesting observation.
\end{p4}

% --------------------------------------------------------------
%     You don't have to mess with anything below this line.
% --------------------------------------------------------------
 
\end{document}
